# Part 4: NLP

## Models Used

I used 2 pre-trained models for NLP. For the project the plan with regard to NLP is to describe an image (caption) and answer questions provided by the user. The two models below will be used to complete these goals. I plan to integrate NLP with the CV portion later to provide as much useful detail about an image as possible.
<br />
NLP Test (bottom of notebook): https://github.com/adadamc/ML2_Project/blob/main/Part%204%20-%20NLP/NLP_Part4.ipynb

### ViLT
(Visual Question Answering) <br />
Link: https://huggingface.co/dandelin/vilt-b32-finetuned-vqa <br />
Paper: https://arxiv.org/abs/2102.03334 <br />
ViLT is a pre-trained model on the VQAv2 dataset. <br />

### BLIP
(Image Captioning) <br />
Link: https://huggingface.co/Salesforce/blip-image-captioning-large <br />
Paper: https://arxiv.org/abs/2201.12086 <br />
BLIP is a Visual Language Processing (VLP) framework. <br />
          
